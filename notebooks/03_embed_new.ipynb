{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('torch.cuda.is_available() ' + str(torch.cuda.is_available()))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2_model_df.pkl\n",
    "INPUT_PATH = os.path.join('..', 'data', 'interim')\n",
    "protein_pairs = pd.read_pickle(os.path.join(INPUT_PATH, '2_model_df.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sequence list\n",
    "sequence_examples = protein_pairs['seqID_phage'].tolist()\n",
    "\n",
    "# TODO: Remove this line\n",
    "# Take just a couple of sequences for testing\n",
    "sequence_examples = sequence_examples[:2]\n",
    "\n",
    "# Replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residue embeddings\n",
    "emb_0 = embedding_repr.last_hidden_state[0, :] # shape (9, 1024)\n",
    "emb_1 = embedding_repr.last_hidden_state[1, :] # shape (9, 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n",
    "emb_1_per_protein = emb_1.mean(dim=0) # shape (1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "OUTPUT_PATH = os.path.join('..', 'data', 'interim')\n",
    "torch.save(embedding_repr, os.path.join(OUTPUT_PATH, '3_embeddings.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt test file \n",
    "with open(\"3_MEGA_TEST.txt\", \"w\") as f:\n",
    "    f.write('emb_0\\n')\n",
    "    f.write('size: ' + str(emb_0.size()) + '\\n')\n",
    "    f.write(str(emb_0))\n",
    "    f.write('\\n\\n')\n",
    "    f.write('emb_1\\n')\n",
    "    f.write('size: ' + str(emb_1.size()) + '\\n')\n",
    "    f.write(str(emb_1))\n",
    "    \n",
    "    f.write(\"03_embed finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
