{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('torch.cuda.is_available() ' + str(torch.cuda.is_available()))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2_model_df.pkl\n",
    "INPUT_PATH = os.path.join('..', 'data', 'interim')\n",
    "protein_pairs = pd.read_pickle(os.path.join(INPUT_PATH, '2_model_df.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sequence list\n",
    "sequence_examples = protein_pairs['seqID_phage'].tolist()\n",
    "\n",
    "# TODO: Remove this line\n",
    "# Take just a couple of sequences for testing\n",
    "n_sequences = 4\n",
    "sequence_examples = sequence_examples[:n_sequences]\n",
    "\n",
    "# Store sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in sequence_examples]\n",
    "\n",
    "# Replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "# You can add max_length parameter to limit the length of the sequence\n",
    "ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)   # sequence\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)  # mask to avoid attention on padding tokens\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residue embeddings\n",
    "# shape (batch_size, sequence_length, 1024)\n",
    "emb = dict()\n",
    "for i in range(n_sequences):\n",
    "    emb[i] = embedding_repr.last_hidden_state[i, 1:sequence_lengths[i]+1]\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "# you can average over the sequence length dimension\n",
    "emb_avg = dict()\n",
    "for i in range(n_sequences):\n",
    "    emb_avg[i] = torch.mean(emb[i], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "OUTPUT_PATH = os.path.join('..', 'data', 'interim')\n",
    "torch.save(embedding_repr, os.path.join(OUTPUT_PATH, '3_embeddings.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a txt file with the embeddings and their shapes\n",
    "with open(\"3_MEGA_TEST.txt\", \"w\") as f:\n",
    "    for i in range(n_sequences):\n",
    "        f.write(str(emb[i].cpu().numpy()) + '\\n')\n",
    "        f.write(str(emb[i].shape) + '\\n')\n",
    "        f.write('seq_len: ' + str(sequence_lengths[i]) + '\\n')\n",
    "    f.write(\"03_embed finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
